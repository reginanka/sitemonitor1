import os
import json
import hashlib
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import requests

from log_utils import log_to_buffer, send_log_to_channel
from site_content import get_schedule_content, take_screenshot_between_elements
from telegram_handler import send_notification

API_BASE_URL = os.getenv("API_BASE_URL")
URL = os.environ.get('URL')
SUBSCRIBE = os.environ.get('SUBSCRIBE')

QUEUES = [(i, j) for i in range(1, 7) for j in range(1, 2 + 1)]

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

CURRENT_FILE = DATA_DIR / "current.json"
PREVIOUS_FILE = DATA_DIR / "previous.json"
HASH_FILE = DATA_DIR / "last_hash.json"


def fetch_schedule(cherga_id: int, pidcherga_id: int) -> Tuple[List[Dict], bool]:
    """
    –¢—è–≥–Ω–µ –≥—Ä–∞—Ñ—ñ–∫ –¥–ª—è –æ–¥–Ω—ñ—î—ó —á–µ—Ä–≥–∏.
    –ü–æ–≤–µ—Ä—Ç–∞—î (–¥–∞–Ω—ñ, is_error).
    """
    resp: Optional[requests.Response] = None
    try:
        params = {"cherga_id": cherga_id, "pidcherga_id": pidcherga_id}
        resp = requests.get(API_BASE_URL, params=params, timeout=10)
        resp.raise_for_status()

        text = resp.text.strip()

        if text.startswith("[") and text.endswith("]"):
            data = json.loads(text)
        else:
            if text.startswith("{"):
                text = f"[{text}]"
            data = json.loads(text)

        if isinstance(data, list):
            return data, False

        log_to_buffer(f"‚ö†Ô∏è –í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–µ —Å–ø–∏—Å–æ–∫ –¥–ª—è {cherga_id}.{pidcherga_id}")
        return [], False

    except Exception as e:
        body = resp.text[:200] if resp is not None else ""
        log_to_buffer(
            f"‚ùå –ü–æ–º–∏–ª–∫–∞ {cherga_id}.{pidcherga_id}: {e}. "
            f"–§—Ä–∞–≥–º–µ–Ω—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: {body}"
        )
        return [], True


def fetch_all_schedules() -> Tuple[Dict[str, List[Dict]], Dict[str, bool]]:
    """–ü–æ–≤–µ—Ä—Ç–∞—î (–¥–∞–Ω—ñ, —Å–ª–æ–≤–Ω–∏–∫ –ø–æ–º–∏–ª–æ–∫)."""
    all_schedules: Dict[str, List[Dict]] = {}
    has_error: Dict[str, bool] = {}
    log_to_buffer("üì° –ó–∞–≤–∞–Ω—Ç–∞–∂—É—é –≥—Ä–∞—Ñ—ñ–∫–∏ –ø–æ –≤—Å—ñ—Ö —á–µ—Ä–≥–∞—Ö...")

    for cherga_id, pidcherga_id in QUEUES:
        queue_key = f"{cherga_id}.{pidcherga_id}"
        schedule, is_error = fetch_schedule(cherga_id, pidcherga_id)
        all_schedules[queue_key] = schedule
        has_error[queue_key] = is_error
        error_note = " [–ø–æ–º–∏–ª–∫–∞ API]" if is_error else ""
        log_to_buffer(f"  ‚úì {queue_key}: {len(schedule)} –∑–∞–ø–∏—Å—ñ–≤{error_note}")

    return all_schedules, has_error


def save_json(data, path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def load_json(path: Path):
    if not path.exists():
        return {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}


def calculate_hash(obj) -> str:
    json_str = json.dumps(obj, sort_keys=True, ensure_ascii=False)
    return hashlib.md5(json_str.encode("utf-8")).hexdigest()


def normalize_record(rec: Dict, cherga_id: int, pidcherga_id: int) -> Dict:
    """–ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø–∏—Å—É."""
    date = rec.get("date", "")
    span = rec.get("span", "")
    color = rec.get("color", "").strip().lower()
    return {
        "cherga": cherga_id,
        "pidcherga": pidcherga_id,
        "queue_key": f"{cherga_id}.{pidcherga_id}",
        "date": date,
        "span": span,
        "color": color,
    }


def build_state(
    raw_schedules: Dict[str, List[Dict]],
    has_error: Dict[str, bool],
) -> Tuple[
    Dict[str, List[Dict]],                    # norm_by_queue
    Dict[str, str],                           # main_hashes
    Dict[str, Dict[str, Dict[str, str]]]      # span_hashes[queue][date][span]
]:
    """
    –ë—É–¥—É—î –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π —Å—Ç–∞–Ω –∑ —Ö–µ—à–∞–º–∏ –ø–æ —ñ–Ω—Ç–µ—Ä–≤–∞–ª–∞—Ö.
    """
    norm_by_queue: Dict[str, List[Dict]] = {}
    main_hashes: Dict[str, str] = {}
    span_hashes: Dict[str, Dict[str, Dict[str, str]]] = {}

    for queue_key, schedule in raw_schedules.items():
        if has_error.get(queue_key, False):
            continue

        cherga_id, pidcherga_id = map(int, queue_key.split("."))
        norm_list: List[Dict] = []

        for rec in schedule:
            nrec = normalize_record(rec, cherga_id, pidcherga_id)
            norm_list.append(nrec)

        norm_list.sort(key=lambda r: (r["date"], r["span"]))
        norm_by_queue[queue_key] = norm_list

        # –ì–æ–ª–æ–≤–Ω–∏–π —Ö–µ—à —á–µ—Ä–≥–∏ ‚Äî –≤—ñ–¥ color –∫–æ–∂–Ω–æ–≥–æ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É
        main_hash_data = [{"date": r["date"], "span": r["span"], "color": r["color"]} for r in norm_list]
        main_hashes[queue_key] = calculate_hash(main_hash_data)

        # –•–µ—à—ñ –ø–æ –∫–æ–∂–Ω–æ–º—É —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É
        sh: Dict[str, Dict[str, str]] = {}
        for rec in norm_list:
            d = rec["date"]
            span = rec["span"]
            if d not in sh:
                sh[d] = {}
            sh[d][span] = calculate_hash({"color": rec["color"]})
        
        span_hashes[queue_key] = sh

    return norm_by_queue, main_hashes, span_hashes


def load_last_state():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ö–µ—à—ñ –∑ last_hash.json + –¥–∞–Ω—ñ –∑ previous.json"""
    hash_data = load_json(HASH_FILE)
    prev_norm = load_json(PREVIOUS_FILE)
    
    return {
        "timestamp": hash_data.get("timestamp"),
        "main_hashes": hash_data.get("main_hashes", {}),
        "span_hashes": hash_data.get("span_hashes", {}),
        "norm_by_queue": prev_norm,
    }


def save_state(
    main_hashes: Dict[str, str],
    span_hashes: Dict[str, Dict[str, Dict[str, str]]],
    timestamp: str
) -> None:
    """–ó–±–µ—Ä—ñ–≥–∞—î —Ç—ñ–ª—å–∫–∏ —Ö–µ—à—ñ –≤ last_hash.json"""
    data = {
        "timestamp": timestamp,
        "main_hashes": main_hashes,
        "span_hashes": span_hashes,
    }
    save_json(data, HASH_FILE)

def parse_span(span: str) -> Tuple[str, str]:
    """0000-0030 –∞–±–æ 00:00-00:30 -> (00:00, 00:30)"""
    if not span or "-" not in span:
        return ("", "")
    start, end = span.split("-")
    # –Ø–∫—â–æ –≤–∂–µ —î –¥–≤–æ–∫—Ä–∞–ø–∫–∞, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —è–∫ —î
    if ":" in start:
        return start, end
    return f"{start[:2]}:{start[2:]}", f"{end[:2]}:{end[2:]}"

def group_spans(spans_changes: List[Dict]) -> List[Dict]:
    """–ì—Ä—É–ø—É—î —Å—É—Å—ñ–¥–Ω—ñ —ñ–Ω—Ç–µ—Ä–≤–∞–ª–∏ –∑ –æ–¥–Ω–∞–∫–æ–≤–∏–º —Ç–∏–ø–æ–º –∑–º—ñ–Ω–∏."""
    result: List[Dict] = []
    current: Optional[Dict] = None

    for item in sorted(spans_changes, key=lambda x: x["span"]):
        start_time, end_time = parse_span(item["span"])
        if not current:
            current = {
                "start": start_time,
                "end": end_time,
                "change": item["change"],
            }
        else:
            if current["change"] == item["change"] and current["end"] == start_time:
                current["end"] = end_time
            else:
                result.append(current)
                current = {
                    "start": start_time,
                    "end": end_time,
                    "change": item["change"],
                }

    if current:
        result.append(current)
    return result


def build_diff(
    norm_by_queue: Dict[str, List[Dict]],
    main_hashes: Dict[str, str],
    span_hashes: Dict[str, Dict[str, Dict[str, str]]],
    last_state: Dict,
) -> Dict:
    last_main = last_state.get("main_hashes", {})
    last_span = last_state.get("span_hashes", {})
    last_norm = last_state.get("norm_by_queue", {})

    diff = {
        "queues": [],
        "per_queue": {},
    }

    for queue_key, cur_main_hash in main_hashes.items():
        old_main_hash = last_main.get(queue_key)
        
        if old_main_hash is None:
            log_to_buffer(f"‚ÑπÔ∏è –ü–µ—Ä—à–∏–π –∑–∞–ø—É—Å–∫ –¥–ª—è {queue_key}, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ")
            continue
        
        if old_main_hash == cur_main_hash:
            continue

        # –Ñ –∑–º—ñ–Ω–∏ ‚Äî —à—É–∫–∞—î–º–æ –¥–µ—Ç–∞–ª—ñ
        log_to_buffer(f"üîç –ê–Ω–∞–ª—ñ–∑—É—é –∑–º—ñ–Ω–∏ –¥–ª—è {queue_key}")
        cur_sh = span_hashes.get(queue_key, {})
        old_sh = last_span.get(queue_key, {})
        
        if not old_sh:
            log_to_buffer(f"‚ÑπÔ∏è –ù–µ–º–∞—î –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö span_hashes –¥–ª—è {queue_key}, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ")
            continue

        new_dates = sorted(d for d in cur_sh.keys() if d not in old_sh)
        if new_dates:
            log_to_buffer(f"  üìÖ –ù–æ–≤—ñ –¥–∞—Ç–∏: {new_dates}")
        
        changed_dates = {}

        cur_items = norm_by_queue.get(queue_key, [])
        old_items_list = last_norm.get(queue_key, [])

        for d in cur_sh.keys():
            if d in new_dates:
                continue
            
            # –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ —Ö–µ—à—ñ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—ñ–≤ –¥–ª—è —Ü—ñ—î—ó –¥–∞—Ç–∏
            cur_spans = cur_sh.get(d, {})
            old_spans = old_sh.get(d, {})
            
            changes_for_date = []
            
            for span, cur_span_hash in cur_spans.items():
                old_span_hash = old_spans.get(span)
                if old_span_hash == cur_span_hash:
                    continue
                
                # –•–µ—à —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É –∑–º—ñ–Ω–∏–≤—Å—è
                log_to_buffer(f"  üîÑ –Ü–Ω—Ç–µ—Ä–≤–∞–ª {span} –¥–∞—Ç–∞ {d}: —Ö–µ—à –∑–º—ñ–Ω–∏–≤—Å—è")
                
                # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Å—Ç–∞—Ä–∏–π —ñ –Ω–æ–≤–∏–π –∑–∞–ø–∏—Å
                new_rec = next((r for r in cur_items if r["date"] == d and r["span"] == span), None)
                old_rec = next((r for r in old_items_list if r["date"] == d and r["span"] == span), None)
                
                if new_rec and old_rec:
                    log_to_buffer(f"    –°—Ç–∞—Ä–∏–π: color={old_rec['color']}, –ù–æ–≤–∏–π: color={new_rec['color']}")
                    if new_rec["color"] != old_rec["color"]:
                        change = "added" if new_rec["color"] == "red" else "removed"
                        changes_for_date.append({"span": span, "change": change})
                        log_to_buffer(f"    ‚úÖ –ó–º—ñ–Ω–∞: {change}")
                else:
                    log_to_buffer(f"    ‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å: new_rec={bool(new_rec)}, old_rec={bool(old_rec)}")

            if changes_for_date:
                grouped = group_spans(changes_for_date)
                changed_dates[d] = grouped
                log_to_buffer(f"  ‚úÖ –î–ª—è –¥–∞—Ç–∏ {d} –∑–Ω–∞–π–¥–µ–Ω–æ {len(changes_for_date)} –∑–º—ñ–Ω")

        if new_dates or changed_dates:
            diff["queues"].append(queue_key)
            diff["per_queue"][queue_key] = {
                "new_dates": new_dates,
                "changed_dates": changed_dates,
            }
            log_to_buffer(f"‚úÖ –î–æ–¥–∞–Ω–æ {queue_key} –¥–æ diff")
        else:
            log_to_buffer(f"‚ö†Ô∏è –•–µ—à –∑–º—ñ–Ω–∏–≤—Å—è –¥–ª—è {queue_key}, –∞–ª–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –∑–º—ñ–Ω–∏ –Ω–µ –≤–∏—è–≤–ª–µ–Ω—ñ")

    return diff


def build_notification_text(diff: Dict, url: str, subscribe: str, update_str: str) -> str:
    queues = sorted(diff["queues"])
    any_new = False
    any_changed = False
    
    # –°–ø–æ—á–∞—Ç–∫—É –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–∏–ø–∏ –∑–º—ñ–Ω
    for q in queues:
        info = diff["per_queue"].get(q, {})
        if info.get("new_dates"):
            any_new = True
        if info.get("changed_dates"):
            any_changed = True
    
    # –§–æ—Ä–º—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
    if any_changed and any_new:
        title = f"–î–ª—è —á–µ—Ä–≥ {', '.join(queues)} üîî –û–ù–û–í–õ–ï–ù–ù–Ø –ì–†–ê–§–Ü–ö–ê –í–Ü–î–ö–õ–Æ–ß–ï–ù–¨ + –¥–æ–¥–∞–Ω–∏–π –≥—Ä–∞—Ñ—ñ–∫ –Ω–∞ –∑–∞–≤—Ç—Ä–∞!"
    elif any_changed:
        title = f"–î–ª—è —á–µ—Ä–≥ {', '.join(queues)} üîî –û–ù–û–í–õ–ï–ù–ù–Ø –ì–†–ê–§–Ü–ö–ê –í–Ü–î–ö–õ–Æ–ß–ï–ù–¨"
    elif any_new:
        title = "üîî–î–æ–¥–∞–Ω–æ –Ω–æ–≤–∏–π –≥—Ä–∞—Ñ—ñ–∫ –Ω–∞ –∑–∞–≤—Ç—Ä–∞!"
    else:
        title = ""
    
    parts: List[str] = []
    if title:
        parts.append(title)
        parts.append("‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è")
    
    # –ì—Ä—É–ø—É—î–º–æ –∑–º—ñ–Ω–∏ –ø–æ —á–µ—Ä–≥–∞—Ö
    queue_blocks: List[str] = []
    for q in queues:
        info = diff["per_queue"].get(q, {})
        queue_lines: List[str] = []
        
        # –î–æ–¥–∞—î–º–æ –≤—Å—ñ –∑–º—ñ–Ω–∏ –¥–ª—è —Ü—ñ—î—ó —á–µ—Ä–≥–∏
        for d, ranges in sorted(info.get("changed_dates", {}).items()):
            for r in ranges:
                action = "ü™´–¥–æ–¥–∞–ª–∏ –≤—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è ‚ùå" if r["change"] == "added" else "üîã—Å–∫–∞—Å—É–≤–∞–ª–∏ –≤—ñ–¥–∫–ª—é—á–µ–Ω–Ω—èüí°"
                queue_lines.append(f" {d} {r['start']}-{r['end']} {action}")
        
        # –Ø–∫—â–æ —î –∑–º—ñ–Ω–∏ –¥–ª—è —á–µ—Ä–≥–∏, –¥–æ–¥–∞—î–º–æ –±–ª–æ–∫
        if queue_lines:
            queue_block = f"‚ñ∂Ô∏è –ß–µ—Ä–≥–∞ {q}:\n" + "\n".join(queue_lines)
            queue_blocks.append(queue_block)
    
    if queue_blocks:
        parts.append("\n\n".join(queue_blocks))
    
    parts.append(f'<a href="{URL}">üîó –ü–µ—Ä–µ–≥–ª—è–Ω—É—Ç–∏ –≥—Ä–∞—Ñ—ñ–∫ –Ω–∞ —Å–∞–π—Ç—ñ </a>')
    
    if update_str:
        parts.append(update_str)
    
    parts.append(f'<a href="{SUBSCRIBE}">‚ö° –ü–Ü–î–ü–ò–°–ê–¢–ò–°–Ø ‚ö°</a>')
    
    return "\n\n".join(parts)



def main():
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_to_buffer("=" * 60)
    log_to_buffer(f"üöÄ –°–¢–ê–†–¢ [{timestamp}]")
    log_to_buffer("=" * 60)

    try:
        # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –≥—Ä–∞—Ñ—ñ–∫–∏ –∑ API
        current_schedules, has_error = fetch_all_schedules()
        if not current_schedules:
            log_to_buffer("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—å –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∂–æ–¥–µ–Ω –≥—Ä–∞—Ñ—ñ–∫")
            return

        # 2. –ü–æ–±—É–¥—É–≤–∞—Ç–∏ –ø–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω
        norm_by_queue, current_main_hashes, current_span_hashes = build_state(
            current_schedules, has_error
        )
        log_to_buffer(f"üîê –í–∏—Ç—è–≥–Ω–µ–Ω–æ —Ö–µ—à—ñ –¥–ª—è {len(current_main_hashes)} —á–µ—Ä–≥")

        # 3. –ó–±–µ—Ä–µ–≥—Ç–∏ –ø–æ—Ç–æ—á–Ω—ñ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ
        # –°–ø–æ—á–∞—Ç–∫—É –∫–æ–ø—ñ—é—î–º–æ current ‚Üí previous
        if CURRENT_FILE.exists():
            shutil.copy(CURRENT_FILE, PREVIOUS_FILE)
            log_to_buffer("üìã –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π current.json —Å–∫–æ–ø—ñ–π–æ–≤–∞–Ω–æ –≤ previous.json")
        
        save_json(norm_by_queue, CURRENT_FILE)
        log_to_buffer("üíæ –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ data/current.json")

        # 4. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å—Ç–∞–Ω
        last_state = load_last_state()
        log_to_buffer("üìã –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å—Ç–∞–Ω")

        # 5. –ü–æ–±—É–¥—É–≤–∞—Ç–∏ diff
        diff = build_diff(norm_by_queue, current_main_hashes, current_span_hashes, last_state)

        if not diff["queues"]:
            log_to_buffer("‚úÖ –î–∞–Ω—ñ –ø–æ –≤—Å—ñ—Ö —á–µ—Ä–≥–∞—Ö –Ω–µ –∑–º—ñ–Ω–∏–ª–∏—Å—è")
            save_state(current_main_hashes, current_span_hashes, timestamp)
            return

        log_to_buffer(f"üîî –ó–º—ñ–Ω–∏ –≤–∏—è–≤–ª–µ–Ω–æ –¥–ª—è: {', '.join(diff['queues'])}")

        # 6. –û—Ç—Ä–∏–º–∞—Ç–∏ –¥–∞—Ç—É –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ —Å–∞–π—Ç—É
        _, date_content = get_schedule_content()

        # 7. –°–∫—Ä—ñ–Ω—à–æ—Ç —ñ–∑ —Å–∞–π—Ç—É
        screenshot_path, screenshot_hash = take_screenshot_between_elements()
        if not screenshot_path:
            log_to_buffer("‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ —Å–∫—Ä—ñ–Ω—à–æ—Ç")

        # 8. –§–æ—Ä–º—É–≤–∞–Ω–Ω—è –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
        final_message = build_notification_text(
            diff,
            URL,
            SUBSCRIBE,
            date_content or "",
        )
 
        # 9. –í—ñ–¥–ø—Ä–∞–≤–∏—Ç–∏ –≤ Telegram
        from pathlib import Path as _Path
        img_path = _Path(screenshot_path) if screenshot_path else None
        ok = send_notification(final_message, img_path)
        if ok:
            log_to_buffer("‚úÖ –ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∑ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è–º –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ –∫–∞–Ω–∞–ª")
        else:
            log_to_buffer("‚ùå –ü–æ–º–∏–ª–∫–∞ –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—è –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤ –∫–∞–Ω–∞–ª")

        # 10. –û–Ω–æ–≤–∏—Ç–∏ —Ç—ñ–ª—å–∫–∏ —Ö–µ—à—ñ
        save_state(current_main_hashes, current_span_hashes, timestamp)
        log_to_buffer("üíæ –•–µ—à—ñ –æ–Ω–æ–≤–ª–µ–Ω–æ –≤ data/last_hash.json")

    except Exception as e:
        log_to_buffer(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
    finally:
        send_log_to_channel()
        log_to_buffer("üèÅ –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–æ–±–æ—Ç–∏ —Å–∫—Ä–∏–ø—Ç–∞")


if __name__ == "__main__":
    main()
